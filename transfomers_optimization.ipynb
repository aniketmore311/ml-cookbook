{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# HuggingFace meets `bitsandbytes` for lighter models on GPU for inference\n",
        "\n",
        "## Running T5-11b on Google Colab "
      ],
      "metadata": {
        "id": "Gbr9iYfRy4GZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " <center>\n",
        " <img src=\"https://s3.amazonaws.com/moonup/production/uploads/1659861207959-62441d1d9fdefb55a0b7d12c.png\">\n",
        " </center>"
      ],
      "metadata": {
        "id": "32ShdTnhMoKV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "You can run your own 8-bit model on any HuggingFace ðŸ¤— model with just few lines of code. This notebook shows how to do it with a `T5` model that would usually require 12GB of GPU RAM.\n",
        "Install the dependencies below first!\n"
      ],
      "metadata": {
        "id": "K9uJn2NczFBK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aep1KMF6dqdm"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet bitsandbytes\n",
        "!pip install --quiet --upgrade transformers # Install latest version of transformers\n",
        "!pip install --quiet --upgrade accelerate\n",
        "!pip install --quiet sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers bitsandbytes accelerate"
      ],
      "metadata": {
        "id": "w6YTOisTFDsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Choose your model"
      ],
      "metadata": {
        "id": "ymZwkJUenuV1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rerun this cell if you want to change the model!"
      ],
      "metadata": {
        "id": "_3cMSVC-oCyM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"t5-3b-sharded\" #@param [\"t5-11b-sharded\", \"t5-3b-sharded\"]"
      ],
      "metadata": {
        "id": "3HibeFxJnwq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use 8bit models with `t5-3b-sharded` ðŸ¤—"
      ],
      "metadata": {
        "id": "BejbQStU5Eik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# T5-3b and T5-11B are supported!\n",
        "# We need sharded weights otherwise we get CPU OOM errors\n",
        "model_id=f\"ybelkada/{model_name}\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model_8bit = AutoModelForSeq2SeqLM.from_pretrained(model_id, device_map=\"auto\", load_in_8bit=True)"
      ],
      "metadata": {
        "id": "YJlldexxwnhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check the memory footprint of this model! ðŸª¶"
      ],
      "metadata": {
        "id": "OtvnO-jtM1is"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_8bit.get_memory_footprint()"
      ],
      "metadata": {
        "id": "uDgDNAmYM1Lx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For `t5-3b` the int8 model is about ~2.9GB! whereas the original model has 11GB. For `t5-11b` the int8 model is about ~11GB vs 42GB for the original model.\n",
        "Now let's generate and see the qualitative results of the 8bit model!"
      ],
      "metadata": {
        "id": "TsuYzVoBPMtn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_new_tokens = 50\n",
        "\n",
        "input_ids = tokenizer(\n",
        "    \"translate English to German: Hello my name is Younes and I am a Machine Learning Engineer at Hugging Face\", return_tensors=\"pt\"\n",
        ").input_ids  \n",
        "\n",
        "outputs = model_8bit.generate(input_ids, max_new_tokens=max_new_tokens)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "j1s0spY4icGK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}